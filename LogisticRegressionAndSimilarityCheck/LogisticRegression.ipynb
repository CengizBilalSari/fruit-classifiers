{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45905,"status":"ok","timestamp":1764352389263,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"_VI2K5ZNKr2W","outputId":"a3f2a7cc-dad0-4432-e36f-abd5b580623c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import random\n","import copy\n","import matplotlib.pyplot as plt\n","import time\n","drive.mount('/content/drive')\n","MY_DRIVE_PATH = \"/content/drive/MyDrive/MLProject\"\n","DATA_FOLDER = os.path.join(MY_DRIVE_PATH, 'Data google sheet')\n","PROCESSED_CSV_FILE = os.path.join(DATA_FOLDER, 'Processed_Fruits_Data.csv')\n","ONEHOT_CSV_FILE = os.path.join(DATA_FOLDER, 'One_Hot_Processed_Fruits_Data.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3235,"status":"ok","timestamp":1764352392492,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"z7CGJzaPLY9t"},"outputs":[],"source":["# Initialize\n","df = pd.read_csv(ONEHOT_CSV_FILE, sep = \";\")\n","random.seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":632,"status":"ok","timestamp":1764352393139,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"22E395f0Lwl9","outputId":"054b96d8-3f43-42fb-e7b5-84f5156a1b70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Shape: (2250, 493)\n","Val Shape:   (250, 493)\n","Test Shape:  (500, 493)\n"]}],"source":["# Preprocessing\n","# Removing unnecessary text columns\n","df.drop(columns=[\"Image_path\",\"Text\",\"Label\"], inplace=True)\n","\n","# Keeping number of items same for each class\n","N_TRAIN = 450\n","N_VAL   = 50\n","N_TEST  = 100\n","N_TOTAL = N_TRAIN + N_VAL + N_TEST\n","\n","train_dfs = []\n","val_dfs = []\n","test_dfs = []\n","\n","categories = ['banana', 'tomato', 'apple', 'orange', 'tangerine']\n","for category in categories:\n","    subset = df[df[\"Fruit\"] == category]\n","    subset = subset.sample(N_TOTAL, random_state=42).reset_index(drop=True)\n","\n","    train_subset = subset.iloc[:N_TRAIN]\n","    val_subset   = subset.iloc[N_TRAIN : N_TRAIN + N_VAL]\n","    test_subset  = subset.iloc[N_TRAIN + N_VAL : N_TOTAL]\n","\n","    train_dfs.append(train_subset)\n","    val_dfs.append(val_subset)\n","    test_dfs.append(test_subset)\n","\n","# 3. Concatenating and shuffling\n","df_train = pd.concat(train_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n","df_val   = pd.concat(val_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n","df_test  = pd.concat(test_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","print(f\"Train Shape: {df_train.shape}\")\n","print(f\"Val Shape:   {df_val.shape}\")\n","print(f\"Test Shape:  {df_test.shape}\")\n","\n","# Normalization\n","numerical_cols = [\"Weight\",\"Price\"]\n","image_cols = [column for column in df_train.columns if \"img\" in column]\n","text_cols = [column for column in df_train.columns if \"text\" in column]\n","categorical_cols = [column for column in df_train.columns if (column not in numerical_cols + image_cols + text_cols) and (column != \"Fruit\")] # We don't want the target\n","columns_to_normalize = numerical_cols + image_cols + text_cols\n","\n","epsilon = 1e-8  # To prevent division by zero\n","for column in columns_to_normalize:\n","    mean = df_train[column].mean()\n","    std = df_train[column].std()\n","    df_train[column] = (df_train[column] - mean) / (std + epsilon)\n","    df_val[column]   = (df_val[column] - mean) / (std + epsilon)\n","    df_test[column]  = (df_test[column] - mean) / (std + epsilon)\n","\n","# Removing the target\n","target_col = 'Fruit'\n","\n","X_train = df_train.drop(columns=[target_col])\n","y_train = df_train[target_col]\n","\n","X_val = df_val.drop(columns=[target_col])\n","y_val = df_val[target_col]\n","\n","X_test = df_test.drop(columns=[target_col])\n","y_test = df_test[target_col]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_mMpakJkVkM4","executionInfo":{"status":"ok","timestamp":1764352393161,"user_tz":-180,"elapsed":7,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"}}},"outputs":[],"source":["def sigmoid(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","def train_logistic_regression(X_train,y_train,X_val,y_val,class_name,selected_cols,epochs=1000,learning_rate=0.001,regularization_lambda=0.0001,patience=5):\n","  train = np.array(X_train[selected_cols])\n","  val = np.array(X_val[selected_cols])\n","  num_features = train.shape[1]\n","  N = len(train)\n","\n","  # Adding a column of ones for the bias term\n","  ones_train  = np.ones((train.shape[0], 1))\n","  train_augmented = np.concatenate((ones_train, train), axis=1)\n","  ones_val    = np.ones((val.shape[0], 1))\n","  val_augmented   = np.concatenate((ones_val, val), axis=1)\n","\n","  # Initialize weigths\n","  weights = np.zeros(np.shape(train_augmented)[1])\n","\n","  # Creating labels +1 and -1 like the slides\n","  train_labels = np.where(y_train == class_name, 1, -1)\n","  val_labels = np.where(y_val == class_name, 1, -1)\n","\n","  train_losses = []\n","  val_losses = []\n","\n","  # Early stopping\n","  best_val_loss = float('inf')\n","  patience_counter = 0\n","  best_weights = copy.deepcopy(weights)\n","\n","  for epoch in range(epochs):\n","    train_loss = 0\n","    val_loss = 0\n","    # Random shuffling for SGD\n","    indices = np.arange(N)\n","    np.random.shuffle(indices)\n","    for i in range(N):\n","      wTx = np.dot(weights, train_augmented[i])\n","      prediction = sigmoid(wTx)\n","      train_loss += np.log(1 + np.exp(-train_labels[i] * wTx))\n","      loss_gradient = -train_labels[i] * train_augmented[i] / (1 + np.exp(train_labels[i] * wTx))\n","      regularization_gradient = regularization_lambda * weights\n","      gradient = loss_gradient + regularization_gradient\n","      weights = weights - learning_rate * gradient\n","    train_loss /= N\n","    # Calculating all scores at once\n","    val_wTx = np.dot(val_augmented, weights)\n","    val_loss = np.mean(np.log(1 + np.exp(-val_labels * val_wTx)))\n","    if val_loss < best_val_loss:\n","      best_val_loss = val_loss\n","      best_weights = copy.deepcopy(weights)\n","      patience_counter = 0\n","    else:\n","      patience_counter += 1\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    if epoch % 50 == 0:\n","      print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n","    if patience_counter >= patience:\n","      print(\"Stopped with early stopping\")\n","      break\n","  return best_weights, train_losses, val_losses"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xp75Ov9XsdSp","executionInfo":{"status":"ok","timestamp":1764352394505,"user_tz":-180,"elapsed":1340,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"}}},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def train_sklearn_logistic_regression(X_train, y_train, X_val, y_val, class_name,\n","                                      selected_cols=None,\n","                                      C=10000, # lambda = 0.0001\n","                                      max_iter=1000):\n","\n","    X_train_subset = X_train[selected_cols]\n","    X_val_subset   = X_val[selected_cols]\n","\n","    y_train_bin = np.where(y_train == class_name, 1, 0) # Sklearn prefers 0/1 for binary\n","    y_val_bin   = np.where(y_val == class_name, 1, 0)\n","\n","    model = LogisticRegression(\n","        penalty='l2',\n","        C=C,\n","        solver='lbfgs',\n","        max_iter=max_iter,\n","        random_state=42\n","        )\n","\n","    model.fit(X_train_subset, y_train_bin)\n","\n","    # Calculating the loss with our function\n","    train_z = model.decision_function(X_train_subset)\n","    val_z   = model.decision_function(X_val_subset)\n","\n","    # Converting labels to +1/-1\n","    y_train_pm1 = np.where(y_train_bin == 1, 1, -1)\n","    y_val_pm1   = np.where(y_val_bin == 1, 1, -1)\n","\n","    # Calculate mean log-loss manually\n","    train_loss = np.mean(np.log(1 + np.exp(-y_train_pm1 * train_z)))\n","    val_loss   = np.mean(np.log(1 + np.exp(-y_val_pm1 * val_z)))\n","\n","    print(f\"Sklearn Final Results for {class_name}:\")\n","    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","\n","    return model, train_loss, val_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BEqsMmFDlvTc","executionInfo":{"status":"ok","timestamp":1764352394529,"user_tz":-180,"elapsed":22,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"}}},"outputs":[],"source":["def plot_losses(train_losses, val_losses, class_name, features_used=\"all\", implementation=\"ours\"):\n","  plt.plot(train_losses, label='Training Loss')\n","  plt.plot(val_losses, label='Validation Loss')\n","  plt.title(f\"Training curve for class : {class_name}, using {features_used} features\")\n","  plt.savefig(f\"{implementation}_training_curve_{class_name}_{features_used}.png\")"]},{"cell_type":"markdown","metadata":{"id":"joETaQuMlBT_"},"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yp-QQkE8bodC","outputId":"146b55c3-910d-45fe-aa0c-6366d8dcf1e9","executionInfo":{"status":"ok","timestamp":1764353307130,"user_tz":-180,"elapsed":912599,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Training Models using IMAGE features ===\n","  > Training One-vs-All for: banana\n","Epoch 1/1000 - Train Loss: 0.6795 - Val Loss: 0.6703\n","Epoch 51/1000 - Train Loss: 0.4330 - Val Loss: 0.5008\n","Stopped with early stopping\n","Sklearn Final Results for banana:\n","Train Loss: 0.2920 | Val Loss: 1.4148\n","  > Training One-vs-All for: tomato\n","Epoch 1/1000 - Train Loss: 0.6815 - Val Loss: 0.6666\n","Epoch 51/1000 - Train Loss: 0.4493 - Val Loss: 0.4520\n","Epoch 101/1000 - Train Loss: 0.4343 - Val Loss: 0.4396\n","Epoch 151/1000 - Train Loss: 0.4279 - Val Loss: 0.4345\n","Epoch 201/1000 - Train Loss: 0.4237 - Val Loss: 0.4317\n","Epoch 251/1000 - Train Loss: 0.4207 - Val Loss: 0.4301\n","Epoch 301/1000 - Train Loss: 0.4184 - Val Loss: 0.4291\n","Epoch 351/1000 - Train Loss: 0.4164 - Val Loss: 0.4285\n","Epoch 401/1000 - Train Loss: 0.4148 - Val Loss: 0.4281\n","Epoch 451/1000 - Train Loss: 0.4134 - Val Loss: 0.4278\n","Epoch 501/1000 - Train Loss: 0.4121 - Val Loss: 0.4277\n","Epoch 551/1000 - Train Loss: 0.4110 - Val Loss: 0.4276\n","Epoch 601/1000 - Train Loss: 0.4099 - Val Loss: 0.4276\n","Stopped with early stopping\n","Sklearn Final Results for tomato:\n","Train Loss: 0.3746 | Val Loss: 0.4477\n","  > Training One-vs-All for: apple\n","Epoch 1/1000 - Train Loss: 0.6793 - Val Loss: 0.6691\n","Epoch 51/1000 - Train Loss: 0.4446 - Val Loss: 0.4632\n","Epoch 101/1000 - Train Loss: 0.4300 - Val Loss: 0.4445\n","Epoch 151/1000 - Train Loss: 0.4235 - Val Loss: 0.4369\n","Epoch 201/1000 - Train Loss: 0.4192 - Val Loss: 0.4313\n","Epoch 251/1000 - Train Loss: 0.4160 - Val Loss: 0.4269\n","Epoch 301/1000 - Train Loss: 0.4135 - Val Loss: 0.4233\n","Epoch 351/1000 - Train Loss: 0.4114 - Val Loss: 0.4202\n","Epoch 401/1000 - Train Loss: 0.4097 - Val Loss: 0.4176\n","Epoch 451/1000 - Train Loss: 0.4083 - Val Loss: 0.4153\n","Epoch 501/1000 - Train Loss: 0.4070 - Val Loss: 0.4134\n","Epoch 551/1000 - Train Loss: 0.4059 - Val Loss: 0.4116\n","Epoch 601/1000 - Train Loss: 0.4049 - Val Loss: 0.4101\n","Epoch 651/1000 - Train Loss: 0.4040 - Val Loss: 0.4088\n","Epoch 701/1000 - Train Loss: 0.4032 - Val Loss: 0.4075\n","Epoch 751/1000 - Train Loss: 0.4024 - Val Loss: 0.4065\n","Epoch 801/1000 - Train Loss: 0.4018 - Val Loss: 0.4055\n","Epoch 851/1000 - Train Loss: 0.4012 - Val Loss: 0.4046\n","Epoch 901/1000 - Train Loss: 0.4006 - Val Loss: 0.4038\n","Epoch 951/1000 - Train Loss: 0.4001 - Val Loss: 0.4031\n","Sklearn Final Results for apple:\n","Train Loss: 0.3884 | Val Loss: 0.4101\n","  > Training One-vs-All for: orange\n","Epoch 1/1000 - Train Loss: 0.6811 - Val Loss: 0.6677\n","Epoch 51/1000 - Train Loss: 0.4409 - Val Loss: 0.4308\n","Epoch 101/1000 - Train Loss: 0.4266 - Val Loss: 0.4156\n","Epoch 151/1000 - Train Loss: 0.4207 - Val Loss: 0.4103\n","Epoch 201/1000 - Train Loss: 0.4170 - Val Loss: 0.4077\n","Epoch 251/1000 - Train Loss: 0.4143 - Val Loss: 0.4062\n","Epoch 301/1000 - Train Loss: 0.4121 - Val Loss: 0.4052\n","Epoch 351/1000 - Train Loss: 0.4103 - Val Loss: 0.4045\n","Epoch 401/1000 - Train Loss: 0.4087 - Val Loss: 0.4040\n","Epoch 451/1000 - Train Loss: 0.4074 - Val Loss: 0.4036\n","Epoch 501/1000 - Train Loss: 0.4062 - Val Loss: 0.4034\n","Epoch 551/1000 - Train Loss: 0.4051 - Val Loss: 0.4032\n","Epoch 601/1000 - Train Loss: 0.4041 - Val Loss: 0.4031\n","Epoch 651/1000 - Train Loss: 0.4033 - Val Loss: 0.4031\n","Stopped with early stopping\n","Sklearn Final Results for orange:\n","Train Loss: 0.3818 | Val Loss: 0.6103\n","  > Training One-vs-All for: tangerine\n","Epoch 1/1000 - Train Loss: 0.6814 - Val Loss: 0.6694\n","Epoch 51/1000 - Train Loss: 0.4464 - Val Loss: 0.4870\n","Stopped with early stopping\n","Sklearn Final Results for tangerine:\n","Train Loss: 0.3645 | Val Loss: 0.3762\n","\n","=== Training Models using TEXT features ===\n","  > Training One-vs-All for: banana\n","Epoch 1/1000 - Train Loss: 0.6189 - Val Loss: 0.5682\n","Epoch 51/1000 - Train Loss: 0.1872 - Val Loss: 0.1925\n","Epoch 101/1000 - Train Loss: 0.1510 - Val Loss: 0.1605\n","Epoch 151/1000 - Train Loss: 0.1355 - Val Loss: 0.1486\n","Epoch 201/1000 - Train Loss: 0.1262 - Val Loss: 0.1424\n","Epoch 251/1000 - Train Loss: 0.1199 - Val Loss: 0.1387\n","Epoch 301/1000 - Train Loss: 0.1152 - Val Loss: 0.1362\n","Epoch 351/1000 - Train Loss: 0.1115 - Val Loss: 0.1345\n","Epoch 401/1000 - Train Loss: 0.1085 - Val Loss: 0.1332\n","Epoch 451/1000 - Train Loss: 0.1060 - Val Loss: 0.1322\n","Epoch 501/1000 - Train Loss: 0.1038 - Val Loss: 0.1315\n","Epoch 551/1000 - Train Loss: 0.1020 - Val Loss: 0.1309\n","Epoch 601/1000 - Train Loss: 0.1003 - Val Loss: 0.1304\n","Epoch 651/1000 - Train Loss: 0.0989 - Val Loss: 0.1300\n","Epoch 701/1000 - Train Loss: 0.0975 - Val Loss: 0.1297\n","Epoch 751/1000 - Train Loss: 0.0963 - Val Loss: 0.1294\n","Epoch 801/1000 - Train Loss: 0.0952 - Val Loss: 0.1292\n","Epoch 851/1000 - Train Loss: 0.0942 - Val Loss: 0.1289\n","Epoch 901/1000 - Train Loss: 0.0933 - Val Loss: 0.1287\n","Epoch 951/1000 - Train Loss: 0.0924 - Val Loss: 0.1286\n","Sklearn Final Results for banana:\n","Train Loss: 0.0209 | Val Loss: 0.7019\n","  > Training One-vs-All for: tomato\n","Epoch 1/1000 - Train Loss: 0.6109 - Val Loss: 0.5513\n","Epoch 51/1000 - Train Loss: 0.1552 - Val Loss: 0.1641\n","Epoch 101/1000 - Train Loss: 0.1186 - Val Loss: 0.1318\n","Epoch 151/1000 - Train Loss: 0.1038 - Val Loss: 0.1196\n","Epoch 201/1000 - Train Loss: 0.0953 - Val Loss: 0.1132\n","Epoch 251/1000 - Train Loss: 0.0896 - Val Loss: 0.1093\n","Epoch 301/1000 - Train Loss: 0.0854 - Val Loss: 0.1067\n","Epoch 351/1000 - Train Loss: 0.0821 - Val Loss: 0.1049\n","Epoch 401/1000 - Train Loss: 0.0795 - Val Loss: 0.1036\n","Epoch 451/1000 - Train Loss: 0.0772 - Val Loss: 0.1026\n","Epoch 501/1000 - Train Loss: 0.0754 - Val Loss: 0.1018\n","Epoch 551/1000 - Train Loss: 0.0737 - Val Loss: 0.1012\n","Epoch 601/1000 - Train Loss: 0.0723 - Val Loss: 0.1008\n","Epoch 651/1000 - Train Loss: 0.0710 - Val Loss: 0.1004\n","Epoch 701/1000 - Train Loss: 0.0698 - Val Loss: 0.1001\n","Epoch 751/1000 - Train Loss: 0.0687 - Val Loss: 0.0999\n","Epoch 801/1000 - Train Loss: 0.0677 - Val Loss: 0.0997\n","Epoch 851/1000 - Train Loss: 0.0668 - Val Loss: 0.0995\n","Epoch 901/1000 - Train Loss: 0.0660 - Val Loss: 0.0994\n","Epoch 951/1000 - Train Loss: 0.0652 - Val Loss: 0.0993\n","Sklearn Final Results for tomato:\n","Train Loss: 0.0037 | Val Loss: 1.0687\n","  > Training One-vs-All for: apple\n","Epoch 1/1000 - Train Loss: 0.6223 - Val Loss: 0.5555\n","Epoch 51/1000 - Train Loss: 0.1601 - Val Loss: 0.1568\n","Epoch 101/1000 - Train Loss: 0.1345 - Val Loss: 0.1301\n","Epoch 151/1000 - Train Loss: 0.1238 - Val Loss: 0.1199\n","Epoch 201/1000 - Train Loss: 0.1174 - Val Loss: 0.1144\n","Epoch 251/1000 - Train Loss: 0.1129 - Val Loss: 0.1109\n","Epoch 301/1000 - Train Loss: 0.1094 - Val Loss: 0.1084\n","Epoch 351/1000 - Train Loss: 0.1066 - Val Loss: 0.1066\n","Epoch 401/1000 - Train Loss: 0.1043 - Val Loss: 0.1050\n","Epoch 451/1000 - Train Loss: 0.1023 - Val Loss: 0.1038\n","Epoch 501/1000 - Train Loss: 0.1005 - Val Loss: 0.1027\n","Epoch 551/1000 - Train Loss: 0.0989 - Val Loss: 0.1018\n","Epoch 601/1000 - Train Loss: 0.0974 - Val Loss: 0.1009\n","Epoch 651/1000 - Train Loss: 0.0961 - Val Loss: 0.1002\n","Epoch 701/1000 - Train Loss: 0.0949 - Val Loss: 0.0995\n","Epoch 751/1000 - Train Loss: 0.0938 - Val Loss: 0.0989\n","Epoch 801/1000 - Train Loss: 0.0927 - Val Loss: 0.0983\n","Epoch 851/1000 - Train Loss: 0.0917 - Val Loss: 0.0978\n","Epoch 901/1000 - Train Loss: 0.0908 - Val Loss: 0.0973\n","Epoch 951/1000 - Train Loss: 0.0899 - Val Loss: 0.0968\n","Sklearn Final Results for apple:\n","Train Loss: 0.0007 | Val Loss: 1.1501\n","  > Training One-vs-All for: orange\n","Epoch 1/1000 - Train Loss: 0.6274 - Val Loss: 0.5835\n","Epoch 51/1000 - Train Loss: 0.2630 - Val Loss: 0.2933\n","Epoch 101/1000 - Train Loss: 0.2307 - Val Loss: 0.2695\n","Epoch 151/1000 - Train Loss: 0.2170 - Val Loss: 0.2608\n","Epoch 201/1000 - Train Loss: 0.2088 - Val Loss: 0.2562\n","Epoch 251/1000 - Train Loss: 0.2032 - Val Loss: 0.2532\n","Epoch 301/1000 - Train Loss: 0.1989 - Val Loss: 0.2511\n","Epoch 351/1000 - Train Loss: 0.1955 - Val Loss: 0.2495\n","Epoch 401/1000 - Train Loss: 0.1927 - Val Loss: 0.2483\n","Epoch 451/1000 - Train Loss: 0.1904 - Val Loss: 0.2473\n","Epoch 501/1000 - Train Loss: 0.1884 - Val Loss: 0.2466\n","Epoch 551/1000 - Train Loss: 0.1866 - Val Loss: 0.2459\n","Epoch 601/1000 - Train Loss: 0.1850 - Val Loss: 0.2454\n","Epoch 651/1000 - Train Loss: 0.1836 - Val Loss: 0.2449\n","Epoch 701/1000 - Train Loss: 0.1823 - Val Loss: 0.2446\n","Epoch 751/1000 - Train Loss: 0.1811 - Val Loss: 0.2443\n","Epoch 801/1000 - Train Loss: 0.1800 - Val Loss: 0.2440\n","Epoch 851/1000 - Train Loss: 0.1790 - Val Loss: 0.2438\n","Epoch 901/1000 - Train Loss: 0.1781 - Val Loss: 0.2436\n","Epoch 951/1000 - Train Loss: 0.1772 - Val Loss: 0.2434\n","Sklearn Final Results for orange:\n","Train Loss: 0.0540 | Val Loss: 2.1583\n","  > Training One-vs-All for: tangerine\n","Epoch 1/1000 - Train Loss: 0.6393 - Val Loss: 0.5958\n","Epoch 51/1000 - Train Loss: 0.2504 - Val Loss: 0.2699\n","Epoch 101/1000 - Train Loss: 0.2126 - Val Loss: 0.2390\n","Epoch 151/1000 - Train Loss: 0.1966 - Val Loss: 0.2269\n","Epoch 201/1000 - Train Loss: 0.1873 - Val Loss: 0.2203\n","Epoch 251/1000 - Train Loss: 0.1809 - Val Loss: 0.2160\n","Epoch 301/1000 - Train Loss: 0.1762 - Val Loss: 0.2130\n","Epoch 351/1000 - Train Loss: 0.1725 - Val Loss: 0.2107\n","Epoch 401/1000 - Train Loss: 0.1695 - Val Loss: 0.2089\n","Epoch 451/1000 - Train Loss: 0.1670 - Val Loss: 0.2074\n","Epoch 501/1000 - Train Loss: 0.1649 - Val Loss: 0.2061\n","Epoch 551/1000 - Train Loss: 0.1630 - Val Loss: 0.2050\n","Epoch 601/1000 - Train Loss: 0.1613 - Val Loss: 0.2041\n","Epoch 651/1000 - Train Loss: 0.1598 - Val Loss: 0.2032\n","Epoch 701/1000 - Train Loss: 0.1585 - Val Loss: 0.2025\n","Epoch 751/1000 - Train Loss: 0.1572 - Val Loss: 0.2018\n","Epoch 801/1000 - Train Loss: 0.1561 - Val Loss: 0.2012\n","Epoch 851/1000 - Train Loss: 0.1550 - Val Loss: 0.2006\n","Epoch 901/1000 - Train Loss: 0.1541 - Val Loss: 0.2001\n","Epoch 951/1000 - Train Loss: 0.1532 - Val Loss: 0.1996\n","Sklearn Final Results for tangerine:\n","Train Loss: 0.0561 | Val Loss: 1.7170\n","\n","=== Training Models using CATEGORICAL features ===\n","  > Training One-vs-All for: banana\n","Epoch 1/1000 - Train Loss: 0.6712 - Val Loss: 0.6516\n","Epoch 51/1000 - Train Loss: 0.2431 - Val Loss: 0.2552\n","Epoch 101/1000 - Train Loss: 0.1546 - Val Loss: 0.1697\n","Epoch 151/1000 - Train Loss: 0.1147 - Val Loss: 0.1310\n","Epoch 201/1000 - Train Loss: 0.0927 - Val Loss: 0.1095\n","Epoch 251/1000 - Train Loss: 0.0790 - Val Loss: 0.0958\n","Epoch 301/1000 - Train Loss: 0.0695 - Val Loss: 0.0863\n","Epoch 351/1000 - Train Loss: 0.0627 - Val Loss: 0.0793\n","Epoch 401/1000 - Train Loss: 0.0575 - Val Loss: 0.0737\n","Epoch 451/1000 - Train Loss: 0.0534 - Val Loss: 0.0692\n","Epoch 501/1000 - Train Loss: 0.0500 - Val Loss: 0.0654\n","Epoch 551/1000 - Train Loss: 0.0472 - Val Loss: 0.0622\n","Epoch 601/1000 - Train Loss: 0.0448 - Val Loss: 0.0594\n","Epoch 651/1000 - Train Loss: 0.0428 - Val Loss: 0.0568\n","Epoch 701/1000 - Train Loss: 0.0410 - Val Loss: 0.0546\n","Epoch 751/1000 - Train Loss: 0.0394 - Val Loss: 0.0525\n","Epoch 801/1000 - Train Loss: 0.0379 - Val Loss: 0.0506\n","Epoch 851/1000 - Train Loss: 0.0366 - Val Loss: 0.0489\n","Epoch 901/1000 - Train Loss: 0.0354 - Val Loss: 0.0473\n","Epoch 951/1000 - Train Loss: 0.0344 - Val Loss: 0.0458\n","Sklearn Final Results for banana:\n","Train Loss: 0.0003 | Val Loss: 0.0004\n","  > Training One-vs-All for: tomato\n","Epoch 1/1000 - Train Loss: 0.6658 - Val Loss: 0.6399\n","Epoch 51/1000 - Train Loss: 0.1812 - Val Loss: 0.1775\n","Epoch 101/1000 - Train Loss: 0.1100 - Val Loss: 0.1066\n","Epoch 151/1000 - Train Loss: 0.0800 - Val Loss: 0.0768\n","Epoch 201/1000 - Train Loss: 0.0635 - Val Loss: 0.0605\n","Epoch 251/1000 - Train Loss: 0.0531 - Val Loss: 0.0503\n","Epoch 301/1000 - Train Loss: 0.0459 - Val Loss: 0.0432\n","Epoch 351/1000 - Train Loss: 0.0406 - Val Loss: 0.0381\n","Epoch 401/1000 - Train Loss: 0.0366 - Val Loss: 0.0341\n","Epoch 451/1000 - Train Loss: 0.0333 - Val Loss: 0.0310\n","Epoch 501/1000 - Train Loss: 0.0307 - Val Loss: 0.0284\n","Epoch 551/1000 - Train Loss: 0.0285 - Val Loss: 0.0263\n","Epoch 601/1000 - Train Loss: 0.0266 - Val Loss: 0.0245\n","Epoch 651/1000 - Train Loss: 0.0250 - Val Loss: 0.0229\n","Epoch 701/1000 - Train Loss: 0.0236 - Val Loss: 0.0216\n","Epoch 751/1000 - Train Loss: 0.0224 - Val Loss: 0.0204\n","Epoch 801/1000 - Train Loss: 0.0213 - Val Loss: 0.0194\n","Epoch 851/1000 - Train Loss: 0.0203 - Val Loss: 0.0184\n","Epoch 901/1000 - Train Loss: 0.0194 - Val Loss: 0.0176\n","Epoch 951/1000 - Train Loss: 0.0186 - Val Loss: 0.0168\n","Sklearn Final Results for tomato:\n","Train Loss: 0.0002 | Val Loss: 0.0002\n","  > Training One-vs-All for: apple\n","Epoch 1/1000 - Train Loss: 0.6696 - Val Loss: 0.6468\n","Epoch 51/1000 - Train Loss: 0.2645 - Val Loss: 0.2568\n","Epoch 101/1000 - Train Loss: 0.1801 - Val Loss: 0.1742\n","Epoch 151/1000 - Train Loss: 0.1371 - Val Loss: 0.1323\n","Epoch 201/1000 - Train Loss: 0.1115 - Val Loss: 0.1074\n","Epoch 251/1000 - Train Loss: 0.0946 - Val Loss: 0.0909\n","Epoch 301/1000 - Train Loss: 0.0826 - Val Loss: 0.0792\n","Epoch 351/1000 - Train Loss: 0.0736 - Val Loss: 0.0704\n","Epoch 401/1000 - Train Loss: 0.0666 - Val Loss: 0.0636\n","Epoch 451/1000 - Train Loss: 0.0610 - Val Loss: 0.0581\n","Epoch 501/1000 - Train Loss: 0.0564 - Val Loss: 0.0536\n","Epoch 551/1000 - Train Loss: 0.0525 - Val Loss: 0.0498\n","Epoch 601/1000 - Train Loss: 0.0492 - Val Loss: 0.0466\n","Epoch 651/1000 - Train Loss: 0.0463 - Val Loss: 0.0438\n","Epoch 701/1000 - Train Loss: 0.0438 - Val Loss: 0.0414\n","Epoch 751/1000 - Train Loss: 0.0416 - Val Loss: 0.0392\n","Epoch 801/1000 - Train Loss: 0.0396 - Val Loss: 0.0373\n","Epoch 851/1000 - Train Loss: 0.0378 - Val Loss: 0.0356\n","Epoch 901/1000 - Train Loss: 0.0362 - Val Loss: 0.0341\n","Epoch 951/1000 - Train Loss: 0.0348 - Val Loss: 0.0327\n","Sklearn Final Results for apple:\n","Train Loss: 0.0007 | Val Loss: 0.0007\n","  > Training One-vs-All for: orange\n","Epoch 1/1000 - Train Loss: 0.6772 - Val Loss: 0.6619\n","Epoch 51/1000 - Train Loss: 0.3554 - Val Loss: 0.3499\n","Epoch 101/1000 - Train Loss: 0.2941 - Val Loss: 0.2882\n","Epoch 151/1000 - Train Loss: 0.2640 - Val Loss: 0.2579\n","Epoch 201/1000 - Train Loss: 0.2452 - Val Loss: 0.2391\n","Epoch 251/1000 - Train Loss: 0.2322 - Val Loss: 0.2261\n","Epoch 301/1000 - Train Loss: 0.2224 - Val Loss: 0.2164\n","Epoch 351/1000 - Train Loss: 0.2148 - Val Loss: 0.2088\n","Epoch 401/1000 - Train Loss: 0.2087 - Val Loss: 0.2028\n","Epoch 451/1000 - Train Loss: 0.2037 - Val Loss: 0.1978\n","Epoch 501/1000 - Train Loss: 0.1995 - Val Loss: 0.1937\n","Epoch 551/1000 - Train Loss: 0.1960 - Val Loss: 0.1902\n","Epoch 601/1000 - Train Loss: 0.1930 - Val Loss: 0.1872\n","Epoch 651/1000 - Train Loss: 0.1903 - Val Loss: 0.1846\n","Epoch 701/1000 - Train Loss: 0.1880 - Val Loss: 0.1824\n","Epoch 751/1000 - Train Loss: 0.1860 - Val Loss: 0.1804\n","Epoch 801/1000 - Train Loss: 0.1842 - Val Loss: 0.1786\n","Epoch 851/1000 - Train Loss: 0.1827 - Val Loss: 0.1771\n","Epoch 901/1000 - Train Loss: 0.1812 - Val Loss: 0.1757\n","Epoch 951/1000 - Train Loss: 0.1799 - Val Loss: 0.1744\n","Sklearn Final Results for orange:\n","Train Loss: 0.1554 | Val Loss: 0.1499\n","  > Training One-vs-All for: tangerine\n","Epoch 1/1000 - Train Loss: 0.6772 - Val Loss: 0.6621\n","Epoch 51/1000 - Train Loss: 0.3719 - Val Loss: 0.3715\n","Epoch 101/1000 - Train Loss: 0.3154 - Val Loss: 0.3155\n","Epoch 151/1000 - Train Loss: 0.2867 - Val Loss: 0.2868\n","Epoch 201/1000 - Train Loss: 0.2682 - Val Loss: 0.2682\n","Epoch 251/1000 - Train Loss: 0.2549 - Val Loss: 0.2547\n","Epoch 301/1000 - Train Loss: 0.2447 - Val Loss: 0.2443\n","Epoch 351/1000 - Train Loss: 0.2367 - Val Loss: 0.2360\n","Epoch 401/1000 - Train Loss: 0.2301 - Val Loss: 0.2292\n","Epoch 451/1000 - Train Loss: 0.2246 - Val Loss: 0.2235\n","Epoch 501/1000 - Train Loss: 0.2199 - Val Loss: 0.2187\n","Epoch 551/1000 - Train Loss: 0.2159 - Val Loss: 0.2145\n","Epoch 601/1000 - Train Loss: 0.2125 - Val Loss: 0.2109\n","Epoch 651/1000 - Train Loss: 0.2094 - Val Loss: 0.2078\n","Epoch 701/1000 - Train Loss: 0.2067 - Val Loss: 0.2050\n","Epoch 751/1000 - Train Loss: 0.2044 - Val Loss: 0.2025\n","Epoch 801/1000 - Train Loss: 0.2022 - Val Loss: 0.2002\n","Epoch 851/1000 - Train Loss: 0.2003 - Val Loss: 0.1982\n","Epoch 901/1000 - Train Loss: 0.1986 - Val Loss: 0.1964\n","Epoch 951/1000 - Train Loss: 0.1970 - Val Loss: 0.1947\n","Sklearn Final Results for tangerine:\n","Train Loss: 0.1554 | Val Loss: 0.1498\n","\n","=== Training Models using NUMERICAL features ===\n","  > Training One-vs-All for: banana\n","Epoch 1/1000 - Train Loss: 0.6684 - Val Loss: 0.6448\n","Epoch 51/1000 - Train Loss: 0.1338 - Val Loss: 0.1379\n","Epoch 101/1000 - Train Loss: 0.0799 - Val Loss: 0.0846\n","Epoch 151/1000 - Train Loss: 0.0592 - Val Loss: 0.0637\n","Epoch 201/1000 - Train Loss: 0.0479 - Val Loss: 0.0522\n","Epoch 251/1000 - Train Loss: 0.0407 - Val Loss: 0.0448\n","Epoch 301/1000 - Train Loss: 0.0357 - Val Loss: 0.0395\n","Epoch 351/1000 - Train Loss: 0.0319 - Val Loss: 0.0356\n","Epoch 401/1000 - Train Loss: 0.0290 - Val Loss: 0.0325\n","Epoch 451/1000 - Train Loss: 0.0266 - Val Loss: 0.0300\n","Epoch 501/1000 - Train Loss: 0.0247 - Val Loss: 0.0279\n","Epoch 551/1000 - Train Loss: 0.0231 - Val Loss: 0.0262\n","Epoch 601/1000 - Train Loss: 0.0217 - Val Loss: 0.0247\n","Epoch 651/1000 - Train Loss: 0.0205 - Val Loss: 0.0234\n","Epoch 701/1000 - Train Loss: 0.0195 - Val Loss: 0.0222\n","Epoch 751/1000 - Train Loss: 0.0185 - Val Loss: 0.0212\n","Epoch 801/1000 - Train Loss: 0.0177 - Val Loss: 0.0203\n","Epoch 851/1000 - Train Loss: 0.0170 - Val Loss: 0.0195\n","Epoch 901/1000 - Train Loss: 0.0163 - Val Loss: 0.0188\n","Epoch 951/1000 - Train Loss: 0.0157 - Val Loss: 0.0181\n","Sklearn Final Results for banana:\n","Train Loss: 0.0002 | Val Loss: 0.0003\n","  > Training One-vs-All for: tomato\n","Epoch 1/1000 - Train Loss: 0.6829 - Val Loss: 0.6732\n","Epoch 51/1000 - Train Loss: 0.4887 - Val Loss: 0.4881\n","Epoch 101/1000 - Train Loss: 0.4842 - Val Loss: 0.4829\n","Epoch 151/1000 - Train Loss: 0.4837 - Val Loss: 0.4820\n","Epoch 201/1000 - Train Loss: 0.4836 - Val Loss: 0.4818\n","Epoch 251/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 301/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 351/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 401/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 451/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 501/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 551/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 601/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 651/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 701/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 751/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 801/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 851/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 901/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Epoch 951/1000 - Train Loss: 0.4836 - Val Loss: 0.4817\n","Sklearn Final Results for tomato:\n","Train Loss: 0.4836 | Val Loss: 0.4816\n","  > Training One-vs-All for: apple\n","Epoch 1/1000 - Train Loss: 0.6825 - Val Loss: 0.6715\n","Epoch 51/1000 - Train Loss: 0.4765 - Val Loss: 0.4666\n","Epoch 101/1000 - Train Loss: 0.4714 - Val Loss: 0.4596\n","Epoch 151/1000 - Train Loss: 0.4710 - Val Loss: 0.4586\n","Epoch 201/1000 - Train Loss: 0.4710 - Val Loss: 0.4583\n","Epoch 251/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 301/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 351/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 401/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 451/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 501/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 551/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 601/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 651/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 701/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 751/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 801/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 851/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 901/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Epoch 951/1000 - Train Loss: 0.4710 - Val Loss: 0.4582\n","Sklearn Final Results for apple:\n","Train Loss: 0.4709 | Val Loss: 0.4582\n","  > Training One-vs-All for: orange\n","Epoch 1/1000 - Train Loss: 0.6767 - Val Loss: 0.6618\n","Epoch 51/1000 - Train Loss: 0.3011 - Val Loss: 0.3315\n","Epoch 101/1000 - Train Loss: 0.2586 - Val Loss: 0.3002\n","Epoch 151/1000 - Train Loss: 0.2414 - Val Loss: 0.2885\n","Epoch 201/1000 - Train Loss: 0.2315 - Val Loss: 0.2817\n","Epoch 251/1000 - Train Loss: 0.2249 - Val Loss: 0.2769\n","Epoch 301/1000 - Train Loss: 0.2200 - Val Loss: 0.2731\n","Epoch 351/1000 - Train Loss: 0.2161 - Val Loss: 0.2699\n","Epoch 401/1000 - Train Loss: 0.2130 - Val Loss: 0.2671\n","Epoch 451/1000 - Train Loss: 0.2103 - Val Loss: 0.2647\n","Epoch 501/1000 - Train Loss: 0.2081 - Val Loss: 0.2626\n","Epoch 551/1000 - Train Loss: 0.2061 - Val Loss: 0.2607\n","Epoch 601/1000 - Train Loss: 0.2044 - Val Loss: 0.2590\n","Epoch 651/1000 - Train Loss: 0.2029 - Val Loss: 0.2575\n","Epoch 701/1000 - Train Loss: 0.2016 - Val Loss: 0.2561\n","Epoch 751/1000 - Train Loss: 0.2004 - Val Loss: 0.2549\n","Epoch 801/1000 - Train Loss: 0.1993 - Val Loss: 0.2538\n","Epoch 851/1000 - Train Loss: 0.1984 - Val Loss: 0.2528\n","Epoch 901/1000 - Train Loss: 0.1975 - Val Loss: 0.2519\n","Epoch 951/1000 - Train Loss: 0.1967 - Val Loss: 0.2511\n","Sklearn Final Results for orange:\n","Train Loss: 0.1852 | Val Loss: 0.2433\n","  > Training One-vs-All for: tangerine\n","Epoch 1/1000 - Train Loss: 0.6705 - Val Loss: 0.6496\n","Epoch 51/1000 - Train Loss: 0.1815 - Val Loss: 0.1892\n","Epoch 101/1000 - Train Loss: 0.1249 - Val Loss: 0.1332\n","Epoch 151/1000 - Train Loss: 0.1016 - Val Loss: 0.1100\n","Epoch 201/1000 - Train Loss: 0.0886 - Val Loss: 0.0970\n","Epoch 251/1000 - Train Loss: 0.0800 - Val Loss: 0.0886\n","Epoch 301/1000 - Train Loss: 0.0740 - Val Loss: 0.0827\n","Epoch 351/1000 - Train Loss: 0.0694 - Val Loss: 0.0782\n","Epoch 401/1000 - Train Loss: 0.0659 - Val Loss: 0.0748\n","Epoch 451/1000 - Train Loss: 0.0630 - Val Loss: 0.0720\n","Epoch 501/1000 - Train Loss: 0.0606 - Val Loss: 0.0698\n","Epoch 551/1000 - Train Loss: 0.0586 - Val Loss: 0.0679\n","Epoch 601/1000 - Train Loss: 0.0569 - Val Loss: 0.0663\n","Epoch 651/1000 - Train Loss: 0.0555 - Val Loss: 0.0650\n","Epoch 701/1000 - Train Loss: 0.0542 - Val Loss: 0.0638\n","Epoch 751/1000 - Train Loss: 0.0530 - Val Loss: 0.0628\n","Epoch 801/1000 - Train Loss: 0.0520 - Val Loss: 0.0618\n","Epoch 851/1000 - Train Loss: 0.0511 - Val Loss: 0.0610\n","Epoch 901/1000 - Train Loss: 0.0502 - Val Loss: 0.0603\n","Epoch 951/1000 - Train Loss: 0.0495 - Val Loss: 0.0597\n","Sklearn Final Results for tangerine:\n","Train Loss: 0.0328 | Val Loss: 0.0491\n","\n","=== Training Models using ALL features ===\n","  > Training One-vs-All for: banana\n","Epoch 1/1000 - Train Loss: 0.5955 - Val Loss: 0.5299\n","Epoch 51/1000 - Train Loss: 0.0705 - Val Loss: 0.0868\n","Epoch 101/1000 - Train Loss: 0.0398 - Val Loss: 0.0607\n","Epoch 151/1000 - Train Loss: 0.0281 - Val Loss: 0.0523\n","Epoch 201/1000 - Train Loss: 0.0219 - Val Loss: 0.0488\n","Epoch 251/1000 - Train Loss: 0.0180 - Val Loss: 0.0472\n","Epoch 301/1000 - Train Loss: 0.0153 - Val Loss: 0.0465\n","Epoch 351/1000 - Train Loss: 0.0134 - Val Loss: 0.0464\n","Stopped with early stopping\n","Sklearn Final Results for banana:\n","Train Loss: 0.0001 | Val Loss: 0.1479\n","  > Training One-vs-All for: tomato\n","Epoch 1/1000 - Train Loss: 0.5949 - Val Loss: 0.5206\n","Epoch 51/1000 - Train Loss: 0.0895 - Val Loss: 0.0942\n","Epoch 101/1000 - Train Loss: 0.0572 - Val Loss: 0.0625\n","Epoch 151/1000 - Train Loss: 0.0432 - Val Loss: 0.0486\n","Epoch 201/1000 - Train Loss: 0.0350 - Val Loss: 0.0403\n","Epoch 251/1000 - Train Loss: 0.0295 - Val Loss: 0.0347\n","Epoch 301/1000 - Train Loss: 0.0256 - Val Loss: 0.0307\n","Epoch 351/1000 - Train Loss: 0.0226 - Val Loss: 0.0275\n","Epoch 401/1000 - Train Loss: 0.0202 - Val Loss: 0.0250\n","Epoch 451/1000 - Train Loss: 0.0183 - Val Loss: 0.0230\n","Epoch 501/1000 - Train Loss: 0.0167 - Val Loss: 0.0213\n","Epoch 551/1000 - Train Loss: 0.0154 - Val Loss: 0.0199\n","Epoch 601/1000 - Train Loss: 0.0143 - Val Loss: 0.0186\n","Epoch 651/1000 - Train Loss: 0.0133 - Val Loss: 0.0176\n","Epoch 701/1000 - Train Loss: 0.0125 - Val Loss: 0.0166\n","Epoch 751/1000 - Train Loss: 0.0117 - Val Loss: 0.0158\n","Epoch 801/1000 - Train Loss: 0.0111 - Val Loss: 0.0150\n","Epoch 851/1000 - Train Loss: 0.0105 - Val Loss: 0.0144\n","Epoch 901/1000 - Train Loss: 0.0100 - Val Loss: 0.0138\n","Epoch 951/1000 - Train Loss: 0.0095 - Val Loss: 0.0132\n","Sklearn Final Results for tomato:\n","Train Loss: 0.0001 | Val Loss: 0.0136\n","  > Training One-vs-All for: apple\n","Epoch 1/1000 - Train Loss: 0.6064 - Val Loss: 0.5287\n","Epoch 51/1000 - Train Loss: 0.1022 - Val Loss: 0.1029\n","Epoch 101/1000 - Train Loss: 0.0729 - Val Loss: 0.0763\n","Epoch 151/1000 - Train Loss: 0.0584 - Val Loss: 0.0645\n","Epoch 201/1000 - Train Loss: 0.0491 - Val Loss: 0.0573\n","Epoch 251/1000 - Train Loss: 0.0425 - Val Loss: 0.0523\n","Epoch 301/1000 - Train Loss: 0.0375 - Val Loss: 0.0485\n","Epoch 351/1000 - Train Loss: 0.0336 - Val Loss: 0.0455\n","Epoch 401/1000 - Train Loss: 0.0304 - Val Loss: 0.0430\n","Epoch 451/1000 - Train Loss: 0.0278 - Val Loss: 0.0409\n","Epoch 501/1000 - Train Loss: 0.0256 - Val Loss: 0.0391\n","Epoch 551/1000 - Train Loss: 0.0237 - Val Loss: 0.0375\n","Epoch 601/1000 - Train Loss: 0.0221 - Val Loss: 0.0361\n","Epoch 651/1000 - Train Loss: 0.0207 - Val Loss: 0.0348\n","Epoch 701/1000 - Train Loss: 0.0194 - Val Loss: 0.0337\n","Epoch 751/1000 - Train Loss: 0.0183 - Val Loss: 0.0327\n","Epoch 801/1000 - Train Loss: 0.0174 - Val Loss: 0.0318\n","Epoch 851/1000 - Train Loss: 0.0165 - Val Loss: 0.0309\n","Epoch 901/1000 - Train Loss: 0.0157 - Val Loss: 0.0302\n","Epoch 951/1000 - Train Loss: 0.0150 - Val Loss: 0.0294\n","Sklearn Final Results for apple:\n","Train Loss: 0.0003 | Val Loss: 0.1234\n","  > Training One-vs-All for: orange\n","Epoch 1/1000 - Train Loss: 0.6140 - Val Loss: 0.5569\n","Epoch 51/1000 - Train Loss: 0.1252 - Val Loss: 0.1459\n","Epoch 101/1000 - Train Loss: 0.0879 - Val Loss: 0.1139\n","Epoch 151/1000 - Train Loss: 0.0719 - Val Loss: 0.1004\n","Epoch 201/1000 - Train Loss: 0.0624 - Val Loss: 0.0926\n","Epoch 251/1000 - Train Loss: 0.0559 - Val Loss: 0.0873\n","Epoch 301/1000 - Train Loss: 0.0511 - Val Loss: 0.0835\n","Epoch 351/1000 - Train Loss: 0.0473 - Val Loss: 0.0806\n","Epoch 401/1000 - Train Loss: 0.0443 - Val Loss: 0.0783\n","Epoch 451/1000 - Train Loss: 0.0417 - Val Loss: 0.0764\n","Epoch 501/1000 - Train Loss: 0.0396 - Val Loss: 0.0748\n","Epoch 551/1000 - Train Loss: 0.0377 - Val Loss: 0.0734\n","Epoch 601/1000 - Train Loss: 0.0361 - Val Loss: 0.0723\n","Epoch 651/1000 - Train Loss: 0.0346 - Val Loss: 0.0713\n","Epoch 701/1000 - Train Loss: 0.0333 - Val Loss: 0.0705\n","Epoch 751/1000 - Train Loss: 0.0321 - Val Loss: 0.0698\n","Epoch 801/1000 - Train Loss: 0.0311 - Val Loss: 0.0691\n","Epoch 851/1000 - Train Loss: 0.0301 - Val Loss: 0.0686\n","Epoch 901/1000 - Train Loss: 0.0292 - Val Loss: 0.0681\n","Epoch 951/1000 - Train Loss: 0.0284 - Val Loss: 0.0677\n","Sklearn Final Results for orange:\n","Train Loss: 0.0002 | Val Loss: 0.7916\n","  > Training One-vs-All for: tangerine\n","Epoch 1/1000 - Train Loss: 0.6209 - Val Loss: 0.5622\n","Epoch 51/1000 - Train Loss: 0.1000 - Val Loss: 0.1467\n","Epoch 101/1000 - Train Loss: 0.0643 - Val Loss: 0.1237\n","Epoch 151/1000 - Train Loss: 0.0502 - Val Loss: 0.1168\n","Epoch 201/1000 - Train Loss: 0.0423 - Val Loss: 0.1142\n","Epoch 251/1000 - Train Loss: 0.0372 - Val Loss: 0.1134\n","Stopped with early stopping\n","Sklearn Final Results for tangerine:\n","Train Loss: 0.0002 | Val Loss: 1.4714\n","\n","All models trained and stored!\n"]}],"source":["col_labels = [\"image\", \"text\", \"categorical\", \"numerical\", \"all\"]\n","cols_list = [image_cols, text_cols, categorical_cols, numerical_cols, X_train.columns]\n","categories = ['banana', 'tomato', 'apple', 'orange', 'tangerine']\n","\n","trained_models = {label: {} for label in col_labels}\n","sklearn_models = {label: {} for label in col_labels}\n","history = {label: {} for label in col_labels} # For losses\n","runtimes = {label: {} for label in col_labels}\n","\n","for i, feature_name in enumerate(col_labels):\n","    current_cols = cols_list[i]\n","    print(f\"\\n=== Training Models using {feature_name.upper()} features ===\")\n","    for target in categories:\n","        print(f\"  > Training One-vs-All for: {target}\")\n","        start_time = time.time()\n","        weights, train_losses, val_losses = train_logistic_regression(\n","            X_train, y_train,\n","            X_val, y_val,\n","            class_name=target,\n","            selected_cols=current_cols,\n","            epochs=1000,\n","            learning_rate=0.0001,\n","            patience=10\n","        )\n","        end_time = time.time()\n","        our_time = end_time - start_time\n","        trained_models[feature_name][target] = weights\n","        plot_losses(train_losses, val_losses, target, features_used=feature_name, implementation=\"ours\")\n","        plt.close()\n","        start_time = time.time()\n","        sklearn_weigths, sklearn_losses, sklearn_val_losses = train_sklearn_logistic_regression(\n","            X_train, y_train,\n","            X_val, y_val,\n","            class_name=target,\n","            selected_cols=current_cols,\n","            C=10000\n","        )\n","        end_time = time.time()\n","        sk_time = end_time - start_time\n","        history[feature_name][target] = {\n","            'ours': {\n","                'train': train_losses, # This is a list\n","                'val': val_losses\n","            },\n","            'sklearn': {\n","                'train': sklearn_losses, # This is a single float\n","                'val': sklearn_val_losses\n","            }\n","        }\n","        runtimes[feature_name][target] = {\n","            'ours': our_time,\n","            'sklearn': sk_time\n","        }\n","        sklearn_models[feature_name][target] = sklearn_weigths\n","print(\"\\nAll models trained and stored!\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jJ_Qp02HnQI3","executionInfo":{"status":"ok","timestamp":1764353307145,"user_tz":-180,"elapsed":6,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"}}},"outputs":[],"source":["def predict_multiclass_unified(X, models_dict, feature_type, class_names, feature_cols, implementation='ours'):\n","    # Multiclass prediction by taking the one with highest probability.\n","    if isinstance(X, pd.DataFrame):\n","        X_subset = X[feature_cols].values\n","        X_subset_df = X[feature_cols]\n","\n","    all_probs = []\n","    for target in class_names:\n","        if implementation == 'ours':\n","            weights = models_dict[feature_type][target]\n","            # Adding bias\n","            m = X_subset.shape[0]\n","            ones = np.ones((m, 1))\n","            X_augmented = np.concatenate((ones, X_subset), axis=1)\n","\n","            # Calculate Z and Sigmoid\n","            z = np.dot(X_augmented, weights)\n","            prob = sigmoid(z)\n","            all_probs.append(prob)\n","\n","        else:\n","            model = models_dict[feature_type][target]\n","            # Get probability of 1\n","            prob = model.predict_proba(X_subset_df)[:, 1]\n","            all_probs.append(prob)\n","\n","    all_probs = np.array(all_probs).T\n","    # Selecting index with highest probability\n","    predictions_idx = np.argmax(all_probs, axis=1)\n","    predictions_str = [class_names[i] for i in predictions_idx]\n","\n","    return np.array(predictions_str), all_probs"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":490,"status":"ok","timestamp":1764353307643,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"_IUHlkYz5xoZ","outputId":"b5f89c6b-976d-44a3-dad4-17d53f1cb689"},"outputs":[{"output_type":"stream","name":"stdout","text":["IMPL     | FEATURE      | ACC    | F1     | AUC    | TIME (s)\n","-----------------------------------------------------------------\n","ours     | image        | 0.464  | 0.481  | 0.464  | 0.464  | 0.764  | 105.5174\n","sklearn  | image        | 0.586  | 0.585  | 0.586  | 0.579  | 0.810  | 0.9866\n","ours     | text         | 0.820  | 0.828  | 0.820  | 0.822  | 0.974  | 231.4222\n","sklearn  | text         | 0.808  | 0.811  | 0.808  | 0.809  | 0.959  | 12.5014\n","ours     | categorical  | 0.908  | 0.932  | 0.908  | 0.904  | 0.981  | 193.0227\n","sklearn  | categorical  | 0.918  | 0.942  | 0.918  | 0.914  | 0.983  | 0.1090\n","ours     | numerical    | 0.838  | 0.843  | 0.838  | 0.833  | 0.833  | 188.3523\n","sklearn  | numerical    | 0.830  | 0.833  | 0.830  | 0.827  | 0.833  | 0.1474\n","ours     | all          | 0.990  | 0.990  | 0.990  | 0.990  | 0.998  | 175.9349\n","sklearn  | all          | 0.976  | 0.977  | 0.976  | 0.976  | 0.995  | 1.3620\n","\n","=== FINAL HEAD-TO-HEAD COMPARISON ===\n","   Features Implementation  Accuracy  Precision  Recall  F1 Score      AUC  Total Training Time (s)\n","        all           ours     0.990   0.990019   0.990  0.990000 0.997810               175.934866\n","        all        sklearn     0.976   0.976513   0.976  0.976033 0.994665                 1.361982\n","categorical           ours     0.908   0.932320   0.908  0.904397 0.980975               193.022713\n","categorical        sklearn     0.918   0.941844   0.918  0.914403 0.982595                 0.109023\n","      image           ours     0.464   0.480759   0.464  0.463826 0.764205               105.517388\n","      image        sklearn     0.586   0.585203   0.586  0.579393 0.809590                 0.986635\n","  numerical           ours     0.838   0.842802   0.838  0.832867 0.832635               188.352282\n","  numerical        sklearn     0.830   0.832926   0.830  0.827212 0.833015                 0.147375\n","       text           ours     0.820   0.828453   0.820  0.821904 0.973750               231.422195\n","       text        sklearn     0.808   0.811158   0.808  0.809304 0.958840                12.501386\n","Our average train time : 178.84988870620728\n","Sklearn average train time : 3.02128005027771\n","\n","Average Speedup (Sklearn vs Ours): 59.20x faster\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, roc_auc_score,precision_score,recall_score\n","from sklearn.preprocessing import label_binarize\n","import pandas as pd\n","results_table = []\n","implementations = ['ours', 'sklearn']\n","\n","print(f\"{'IMPL':<8} | {'FEATURE':<12} | {'ACC':<6} | {'F1':<6} | {'AUC':<6} | {'TIME (s)':<8}\")\n","print(\"-\" * 65)\n","\n","for feature_name in col_labels:\n","    current_cols = cols_list[col_labels.index(feature_name)]\n","\n","    # Total training time\n","    time_ours = sum([runtimes[feature_name][t]['ours'] for t in categories])\n","    time_sk   = sum([runtimes[feature_name][t]['sklearn'] for t in categories])\n","\n","    dicts_to_test = {\n","        'ours': (trained_models, time_ours),\n","        'sklearn': (sklearn_models, time_sk)\n","    }\n","\n","    for implementation_name, (model_dict, total_time) in dicts_to_test.items():\n","\n","        y_pred, y_probs = predict_multiclass_unified(\n","            X_test,\n","            model_dict,\n","            feature_type=feature_name,\n","            class_names=categories,\n","            feature_cols=current_cols,\n","            implementation=implementation_name\n","        )\n","\n","        # 2. Calculate Metrics\n","        acc = accuracy_score(y_test, y_pred)\n","        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n","        rec  = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n","        f1  = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n","        y_test_bin = label_binarize(y_test, classes=categories)\n","        try:\n","            auc = roc_auc_score(y_test_bin, y_probs, multi_class='ovr', average='weighted')\n","        except ValueError:\n","            auc = 0.0\n","\n","        # AUC Calculation\n","        from sklearn.preprocessing import label_binarize\n","        y_test_bin = label_binarize(y_test, classes=categories)\n","        try:\n","            auc = roc_auc_score(y_test_bin, y_probs, multi_class='ovr', average='weighted')\n","        except ValueError:\n","            auc = 0.0\n","\n","        print(f\"{implementation_name:<8} | {feature_name:<12} | {acc:.3f}  | {prec:.3f}  | {rec:.3f}  | {f1:.3f}  | {auc:.3f}  | {total_time:.4f}\")\n","\n","        results_table.append({\n","            'Implementation': implementation_name,\n","            'Features': feature_name,\n","            'Accuracy': acc,\n","            'Precision': prec,\n","            'Recall': rec,\n","            'F1 Score': f1,\n","            'AUC': auc,\n","            'Total Training Time (s)': total_time\n","        })\n","\n","df_results = pd.DataFrame(results_table)\n","df_results = df_results.sort_values(by=['Features', 'Implementation'])\n","\n","print(\"\\n=== FINAL HEAD-TO-HEAD COMPARISON ===\")\n","print(df_results[['Features', 'Implementation', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC', 'Total Training Time (s)']].to_string(index=False))\n","\n","ours_avg_time = df_results[df_results['Implementation']=='ours']['Total Training Time (s)'].mean()\n","sk_avg_time = df_results[df_results['Implementation']=='sklearn']['Total Training Time (s)'].mean()\n","print(f\"Our average train time : {ours_avg_time}\")\n","print(f\"Sklearn average train time : {sk_avg_time}\")\n","print(f\"\\nAverage Speedup (Sklearn vs Ours): {ours_avg_time / sk_avg_time:.2f}x faster\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1764353307669,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"308fc822","outputId":"7e4b6367-8e45-48e7-cb31-216f1a5e1488"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25 plot files:\n","ours_training_curve_tangerine_text.png\n","ours_training_curve_orange_all.png\n","ours_training_curve_apple_all.png\n","ours_training_curve_tomato_numerical.png\n","ours_training_curve_orange_numerical.png\n","ours_training_curve_apple_image.png\n","ours_training_curve_tomato_categorical.png\n","ours_training_curve_apple_categorical.png\n","ours_training_curve_banana_numerical.png\n","ours_training_curve_banana_all.png\n","ours_training_curve_apple_numerical.png\n","ours_training_curve_tangerine_image.png\n","ours_training_curve_tangerine_categorical.png\n","ours_training_curve_tomato_image.png\n","ours_training_curve_banana_image.png\n","ours_training_curve_apple_text.png\n","ours_training_curve_tomato_all.png\n","ours_training_curve_orange_image.png\n","ours_training_curve_tangerine_numerical.png\n","ours_training_curve_orange_categorical.png\n","ours_training_curve_banana_text.png\n","ours_training_curve_orange_text.png\n","ours_training_curve_banana_categorical.png\n","ours_training_curve_tangerine_all.png\n","ours_training_curve_tomato_text.png\n"]}],"source":["import glob\n","\n","plot_files = glob.glob('*_training_curve_*.png')\n","print(f\"Found {len(plot_files)} plot files:\")\n","for f in plot_files:\n","    print(f)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1764353307689,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"6f15981e","outputId":"607b3424-1600-486a-cc4c-68547833091a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully created 'training_plots.zip' containing 25 files.\n"]}],"source":["import zipfile\n","\n","zip_filename = 'training_plots.zip'\n","with zipfile.ZipFile(zip_filename, 'w') as zipf:\n","    for file in plot_files:\n","        zipf.write(file)\n","\n","print(f\"Successfully created '{zip_filename}' containing {len(plot_files)} files.\")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1764353307803,"user":{"displayName":"Efe Feyzi Mantaroğlu","userId":"06606282886702319342"},"user_tz":-180},"id":"8eb2ab7d","outputId":"c58ed880-6920-40d3-98b7-dbc85dc4617c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_2e8443c7-48ec-479f-a903-bb005f642db4\", \"training_plots.zip\", 610386)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download(zip_filename)"]}],"metadata":{"colab":{"toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}