{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5neu8y_J4Kx"
   },
   "source": [
    "#Training and Evaluation with All Type of Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaAZC_CiKLL5"
   },
   "source": [
    "#### CSV files are obtained from the directory, necessary imports are done other than classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23761,
     "status": "ok",
     "timestamp": 1767633698339,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "oyM5oYVmkuqK",
    "outputId": "cc725a9b-187f-4c9a-abd0-219112c61423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "drive.mount('/content/drive')\n",
    "MY_DRIVE_PATH = \"/content/drive/MyDrive/MLProject_2\"\n",
    "DATA_FOLDER = os.path.join(MY_DRIVE_PATH, 'Data google sheet')\n",
    "PROCESSED_CSV_FILE = os.path.join(DATA_FOLDER, 'Processed_Fruits_Data.csv')\n",
    "ONEHOT_CSV_FILE = os.path.join(DATA_FOLDER, 'One_Hot_Processed_Fruits_Data.csv')\n",
    "PCA_CSV_PATH = os.path.join(DATA_FOLDER, 'PCA_Processed_Fruits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bD1eo1QlC-v"
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "df = pd.read_csv(ONEHOT_CSV_FILE, sep = \";\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erGaW9biKaMH"
   },
   "source": [
    "#### Separation of the test and training dataset\n",
    "##### For each type, it is separated as 150-450, there is no validation set for this job since we used GridSearchCV for hyperparameter selection and it utilizes from cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1767633726850,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "rWrzWtMglEHx",
    "outputId": "6dd938fc-cdd5-4fe7-8193-9887041a27b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (2250, 493)\n",
      "Test Shape:  (750, 493)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# Removing unnecessary text columns\n",
    "df.drop(columns=[\"Image_path\",\"Text\",\"Label\"], inplace=True)\n",
    "\n",
    "# Keeping number of items same for each class\n",
    "N_TRAIN = 450\n",
    "N_TEST  = 150\n",
    "N_TOTAL = N_TRAIN + N_TEST\n",
    "\n",
    "train_dfs = []\n",
    "test_dfs = []\n",
    "\n",
    "categories = ['banana', 'tomato', 'apple', 'orange', 'tangerine']\n",
    "for category in categories:\n",
    "    subset = df[df[\"Fruit\"] == category]\n",
    "    subset = subset.sample(N_TOTAL, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    train_subset = subset.iloc[:N_TRAIN]\n",
    "    test_subset = subset.iloc[N_TRAIN : N_TOTAL ]\n",
    "    train_dfs.append(train_subset)\n",
    "    test_dfs.append(test_subset)\n",
    "\n",
    "# 3. Concatenating and shuffling\n",
    "df_train = pd.concat(train_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test  = pd.concat(test_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train Shape: {df_train.shape}\")\n",
    "print(f\"Test Shape:  {df_test.shape}\")\n",
    "\n",
    "# Normalization\n",
    "numerical_cols = [\"Weight\",\"Price\"]\n",
    "image_cols = [column for column in df_train.columns if \"img\" in column]\n",
    "text_cols = [column for column in df_train.columns if \"text\" in column]\n",
    "categorical_cols = [column for column in df_train.columns if (column not in numerical_cols + image_cols + text_cols) and (column != \"Fruit\")] # We don't want the target\n",
    "columns_to_normalize = numerical_cols + image_cols + text_cols\n",
    "\n",
    "epsilon = 1e-8  # To prevent division by zero\n",
    "for column in columns_to_normalize:\n",
    "    mean = df_train[column].mean()\n",
    "    std = df_train[column].std()\n",
    "    df_train[column] = (df_train[column] - mean) / (std + epsilon)\n",
    "    df_test[column]  = (df_test[column] - mean) / (std + epsilon)\n",
    "\n",
    "# Removing the target\n",
    "target_col = 'Fruit'\n",
    "\n",
    "X_train = df_train.drop(columns=[target_col])\n",
    "y_train = df_train[target_col]\n",
    "\n",
    "X_test = df_test.drop(columns=[target_col])\n",
    "y_test = df_test[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzXtLgoRnd2C"
   },
   "outputs": [],
   "source": [
    "# Classifiers are taken from the library sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPIbnlcrLE36"
   },
   "source": [
    "###### Benchmarking method is used for any type of classifier, we calculates the training time and accuracy of test data inside of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0TC0zTIn07B"
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "def benchmarking(name,model, params):\n",
    "  start_time = time.time()\n",
    "  model.fit(X_train, y_train)\n",
    "  end_time = time.time()\n",
    "\n",
    "  y_pred = model.predict(X_test)\n",
    "  accuracy_on_test = accuracy_score(y_test, model.predict(X_test))\n",
    "  results.append({\n",
    "        \"Classifier\": name,\n",
    "        \"Training time\": round(end_time - start_time, 5),\n",
    "        \"Test Accuracy\": round(accuracy_on_test, 5),\n",
    "        \"Hyperparameters/Choices\": params\n",
    "    })\n",
    "  print(f\"Finished {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37037,
     "status": "ok",
     "timestamp": 1767633770319,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "yAArXNeJqfv2",
    "outputId": "28cea45b-2102-492d-9460-f383e8803c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression (Linear)\n",
    "# We used lambda for the coefficient of 1/2||w|| part at lecture, but sklearn take C for error part not regularization part,\n",
    "# so the C is inverse regularization term (1 /lambda). If C is small, regularization term is dominant.\n",
    "\n",
    "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10,20,30,40]}, cv=5)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "benchmarking(\"Logistic Regression\", lr_grid.best_estimator_, f\"Best C={lr_grid.best_params_['C']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111625,
     "status": "ok",
     "timestamp": 1767633884532,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "01Lhbg7Gqgn1",
    "outputId": "341d6076-f7c9-4471-a3d4-62201bf5cbae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Non-linear Logit\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression (Non-linear)\n",
    "# transformation of x to [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]\n",
    "# since C= 0.1 is the best for linear one, we used it here also, running it for degree 3-4 crashes the RAM.\n",
    "poly_logit = Pipeline([('poly', PolynomialFeatures(degree=2)), ('logit', LogisticRegression(max_iter=1000, C=0.1))])\n",
    "benchmarking(\"Non-linear Logit\", poly_logit, \"Degree=2 Polynomial, C=0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4757,
     "status": "ok",
     "timestamp": 1767633891395,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "Ebo0JPyF3YPq",
    "outputId": "826d86f5-7d90-4279-dd03-be17c8ae6c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Soft-margin SVM\n"
     ]
    }
   ],
   "source": [
    "#Soft-margin SVM (Linear)\n",
    "# In the formula , C is coefficient of the error part (min 0.5||w|| + C*(margin errors) )\n",
    "svm_linear_grid = GridSearchCV(SVC(kernel='linear'), {'C': [0.01, 0.1, 1]}, cv=5)\n",
    "svm_linear_grid.fit(X_train, y_train)\n",
    "benchmarking(\"Soft-margin SVM\", svm_linear_grid.best_estimator_, f\"Best C={svm_linear_grid.best_params_['C']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33467,
     "status": "ok",
     "timestamp": 1767633926530,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "T_wJLKKE3fQE",
    "outputId": "41210c55-12ee-4344-dbbf-6fae4ac8bc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Kernel SVM\n"
     ]
    }
   ],
   "source": [
    "#SVM with Kernel Trick\n",
    "# The RBF Kernel in the lecture is used for kernel trick\n",
    "svm_kernel_grid = GridSearchCV(SVC(kernel='rbf'), {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}, cv=5)\n",
    "svm_kernel_grid.fit(X_train, y_train)\n",
    "benchmarking(\"Kernel SVM\", svm_kernel_grid.best_estimator_, f\"Best {svm_kernel_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1885,
     "status": "ok",
     "timestamp": 1767633930575,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "LtXoFpXU3g6j",
    "outputId": "d9672297-defb-4a72-a321-df0abafb80c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished k-NN\n"
     ]
    }
   ],
   "source": [
    "#k-Nearest Neighbor\n",
    "# without training anything ,we assigned  the majority vote in the \"k\" nearest point for class\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': [3, 5, 7, 9]}, cv=5)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "benchmarking(\"k-NN\", knn_grid.best_estimator_, f\"Best k={knn_grid.best_params_['n_neighbors']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1767633931624,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "h0nO7DJL3jRt",
    "outputId": "11558741-69ed-49df-e6b1-a4661e9450f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "# We have continuous data for parameters like weight price etc, we have to assign a distribution type for them.\n",
    "benchmarking(\"Naive Bayes\", GaussianNB(), \"Gaussian Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100374,
     "status": "ok",
     "timestamp": 1767634033430,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "lv1D8plB3lOG",
    "outputId": "c4743710-17be-4803-abee-435343aeb9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Random Forest\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "# n_estimators= number of trees\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [50, 100, 200]}, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "benchmarking(\"Random Forest\", rf_grid.best_estimator_, f\"Best N={rf_grid.best_params_['n_estimators']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1767634038145,
     "user": {
      "displayName": "Cengiz Sarı",
      "userId": "07073946851309561314"
     },
     "user_tz": -180
    },
    "id": "3a18qkVK3n8Z",
    "outputId": "8adc3d67-a6b6-4680-e331-ebf02bed83b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Classifier  Training time  Test Accuracy         Hyperparameters/Choices\n",
      "    Soft-margin SVM        0.25728        0.98533                      Best C=0.1\n",
      "      Random Forest       10.65200        0.98267                      Best N=200\n",
      "Logistic Regression        1.27567        0.98133                      Best C=0.1\n",
      "         Kernel SVM        0.78344        0.96933 Best {'C': 10, 'gamma': 'auto'}\n",
      "   Non-linear Logit      109.83466        0.96800      Degree=2 Polynomial, C=0.1\n",
      "        Naive Bayes        0.03308        0.91467           Gaussian Distribution\n",
      "               k-NN        0.02130        0.87333                        Best k=3\n"
     ]
    }
   ],
   "source": [
    "# Final Table\n",
    "#Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzfRyQisMZeq"
   },
   "source": [
    "#### **Evaluations of the Results**\n",
    "The hyperparameter selections are performed by providing predetermined values to the GridSearch library, which selects the most accurate one by trying each combination.\n",
    "##### **Training Times**\n",
    "k-NN and Naive Bayes algorithms are taken the least times, and it is the expected result since k-NN has no training part and Naive Bayes is just straightforward probability multiplications.\n",
    "Non-linear Logit is taking the most amount of time. This is  due to the  increase in feature space dimensionality when generating degree-2 polynomial features. Random Forest also takes significant time due to the overhead of creating 200 individual decision trees.\n",
    "\n",
    "##### **Accuracy**\n",
    "Soft-margin SVM achieved the highest accuracy ($98.53%$) with a  $C$ value ($0.1$), suggesting the data is well-separated by a linear boundary.  Random Forest ($98.27%$) and Logistic Regression ($98.13%$) also have a high accuracy rate. It can be concluded that the relationship between features and the target is predominantly linear from the accuracy of the simple Logistic Regression algorithm.\n",
    "Kernel SVM and Non-linear Logit performed slightly worse ($96.93%$ and $96.80%$ respectively) than their linear versions. This might indicate that the  complexity of non-linear transformations might create overfitting or  the decision boundary is naturally linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7VJ6FkfR10r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
