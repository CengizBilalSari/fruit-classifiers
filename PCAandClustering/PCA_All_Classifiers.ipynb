{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPuSpoazxLQTOdNwGg3b9yX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#  Training and Evaluation with All Type of Classifiers with PCA"],"metadata":{"id":"G3Ibm8_pOT8z"}},{"cell_type":"markdown","source":["#### CSV files are obtained from the directory, necessary imports are done other than classifiers.\n","\n"],"metadata":{"id":"ogOoopEgOcML"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"oyM5oYVmkuqK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767634557059,"user_tz":-180,"elapsed":23327,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"53f19a56-d7ce-43b6-ce03-6e0a0db74bf7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import os\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import random\n","import copy\n","import matplotlib.pyplot as plt\n","import time\n","drive.mount('/content/drive')\n","MY_DRIVE_PATH = \"/content/drive/MyDrive/MLProject_2\"\n","DATA_FOLDER = os.path.join(MY_DRIVE_PATH, 'Data google sheet')\n","PROCESSED_CSV_FILE = os.path.join(DATA_FOLDER, 'Processed_Fruits_Data.csv')\n","ONEHOT_CSV_FILE = os.path.join(DATA_FOLDER, 'One_Hot_Processed_Fruits_Data.csv')\n","PCA_CSV_PATH = os.path.join(DATA_FOLDER, 'PCA_Processed_Fruits.csv')"]},{"cell_type":"code","source":["# Initialize\n","df = pd.read_csv(PCA_CSV_PATH, sep = \",\")\n","random.seed(42)"],"metadata":{"id":"5bD1eo1QlC-v","executionInfo":{"status":"ok","timestamp":1767634563340,"user_tz":-180,"elapsed":1327,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### Separation of the test and training dataset\n","##### For each type, it is separated as 150-450, there is no validation set for this job since we used GridSearchCV for hyperparameter selection and it utilizes from cross-validation.\n","\n"],"metadata":{"id":"GLom5MlrOe1-"}},{"cell_type":"code","source":["# Preprocessing\n","# Removing unnecessary text columns\n","\n","# Keeping number of items same for each class\n","N_TRAIN = 450\n","N_TEST  = 150\n","N_TOTAL = N_TRAIN + N_TEST\n","\n","\n","categories = ['banana', 'tomato', 'apple', 'orange', 'tangerine']\n","train_df_pca = []\n","test_df_pca = []\n","\n","for category in categories:\n","    # We use .get() logic or check the name to be safe\n","    subset = df[df[\"Fruit\"] == category]\n","    subset = subset.sample(N_TOTAL, random_state=42).reset_index(drop=True)\n","\n","    train_subset_pca = subset.iloc[:N_TRAIN]\n","    test_subset_pca  = subset.iloc[N_TRAIN : N_TOTAL]\n","\n","    train_df_pca.append(train_subset_pca)\n","    test_df_pca.append(test_subset_pca)\n","\n","# Concatenating and shuffling\n","df_pca_train = pd.concat(train_df_pca).sample(frac=1, random_state=42).reset_index(drop=True)\n","df_pca_test  = pd.concat(test_df_pca).sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Separate Features (X) and Target (y)\n","target_col = 'Fruit'\n","X_train = df_pca_train.drop(columns=[target_col])\n","y_train = df_pca_train[target_col]\n","X_test = df_pca_test.drop(columns=[target_col])\n","y_test = df_pca_test[target_col]\n","\n","print(\"Success!\")\n","print(f\"PCA Train Shape: {X_train.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWrzWtMglEHx","executionInfo":{"status":"ok","timestamp":1767634565001,"user_tz":-180,"elapsed":54,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"a93d231c-3654-401c-cd86-e6c6b6ecf1d8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Success!\n","PCA Train Shape: (2250, 45)\n"]}]},{"cell_type":"code","source":["# Classifiers are taken from the library sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV"],"metadata":{"id":"DzXtLgoRnd2C","executionInfo":{"status":"ok","timestamp":1767634568882,"user_tz":-180,"elapsed":1775,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["###### Benchmarking method is used for any type of classifier, we calculates the training time and accuracy of test data inside of it.\n","\n"],"metadata":{"id":"QOw-tI4POjI1"}},{"cell_type":"code","source":["results=[]\n","def benchmarking(name,model, params):\n","  start_time = time.time()\n","  model.fit(X_train, y_train)\n","  end_time = time.time()\n","\n","  y_pred = model.predict(X_test)\n","  accuracy_on_test = accuracy_score(y_test, model.predict(X_test))\n","  results.append({\n","        \"Classifier\": name,\n","        \"Training time\": round(end_time - start_time, 5),\n","        \"Test Accuracy\": round(accuracy_on_test, 5),\n","        \"Hyperparameters/Choices\": params\n","    })\n","  print(f\"Finished {name}\")\n"],"metadata":{"id":"A0TC0zTIn07B","executionInfo":{"status":"ok","timestamp":1767634570809,"user_tz":-180,"elapsed":7,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Logistic Regression (Linear)\n","# We used lambda for the coefficient of 1/2||w|| part at lecture, but sklearn take C for error part not regularization part,\n","# so the C is inverse regularization term (1 /lambda). If C is small, regularization term is dominant.\n","\n","lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10,20,30,40]}, cv=5)\n","lr_grid.fit(X_train, y_train)\n","benchmarking(\"Logistic Regression\", lr_grid.best_estimator_, f\"Best C={lr_grid.best_params_['C']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAArXNeJqfv2","executionInfo":{"status":"ok","timestamp":1767634600395,"user_tz":-180,"elapsed":27657,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"a39663be-118b-4432-96ce-02729e89bf73"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Logistic Regression\n"]}]},{"cell_type":"code","source":["#Logistic Regression (Non-linear)\n","# transformation of x to [1, x_1, x_2, x_1^2, x_1x_2, x_2^2]\n","# since C= 0.1 is the best for linear one, we used it here also, running it for degree 3-4 crashes the RAM.\n","poly_logit = Pipeline([('poly', PolynomialFeatures(degree=2)), ('logit', LogisticRegression(max_iter=1000, C=0.1))])\n","benchmarking(\"Non-linear Logit\", poly_logit, \"Degree=2 Polynomial, C=0.1\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01Lhbg7Gqgn1","executionInfo":{"status":"ok","timestamp":1767634621576,"user_tz":-180,"elapsed":13405,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"562c203b-6079-4126-8c06-527482f9e5fe"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Non-linear Logit\n"]}]},{"cell_type":"code","source":["#Soft-margin SVM (Linear)\n","# In the formula , C is coefficient of the error part (min 0.5||w|| + C*(margin errors) )\n","svm_linear_grid = GridSearchCV(SVC(kernel='linear'), {'C': [0.01, 0.1, 1]}, cv=5)\n","svm_linear_grid.fit(X_train, y_train)\n","benchmarking(\"Soft-margin SVM\", svm_linear_grid.best_estimator_, f\"Best C={svm_linear_grid.best_params_['C']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ebo0JPyF3YPq","executionInfo":{"status":"ok","timestamp":1767634629688,"user_tz":-180,"elapsed":3000,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"61d53b21-30f8-4551-c5ed-8606860d9cc4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Soft-margin SVM\n"]}]},{"cell_type":"code","source":["#SVM with Kernel Trick\n","# The RBF Kernel in the lecture is used for kernel trick\n","svm_kernel_grid = GridSearchCV(SVC(kernel='rbf'), {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}, cv=5)\n","svm_kernel_grid.fit(X_train, y_train)\n","benchmarking(\"Kernel SVM\", svm_kernel_grid.best_estimator_, f\"Best {svm_kernel_grid.best_params_}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_wJLKKE3fQE","executionInfo":{"status":"ok","timestamp":1767634645965,"user_tz":-180,"elapsed":14645,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"7badbba4-a48f-4b80-f50e-fbd2a1937f52"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Kernel SVM\n"]}]},{"cell_type":"code","source":["#k-Nearest Neighbor\n","# without training anything ,we assigned  the majority vote in the \"k\" nearest point for class\n","knn_grid = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': [3, 5, 7, 9]}, cv=5)\n","knn_grid.fit(X_train, y_train)\n","benchmarking(\"k-NN\", knn_grid.best_estimator_, f\"Best k={knn_grid.best_params_['n_neighbors']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LtXoFpXU3g6j","executionInfo":{"status":"ok","timestamp":1767634648817,"user_tz":-180,"elapsed":422,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"d3e450cf-cf9b-4d3d-d0b2-d5aa097cc18a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished k-NN\n"]}]},{"cell_type":"code","source":["#Naive Bayes\n","# We have continuous data for parameters like weight price etc, we have to assign a distribution type for them.\n","benchmarking(\"Naive Bayes\", GaussianNB(), \"Gaussian Distribution\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h0nO7DJL3jRt","executionInfo":{"status":"ok","timestamp":1767634651784,"user_tz":-180,"elapsed":39,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"735455e1-2f7e-4fff-d430-01c274d8d73b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Naive Bayes\n"]}]},{"cell_type":"code","source":["#Random Forest\n","# n_estimators= number of trees\n","rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': [50, 100, 200]}, cv=5)\n","rf_grid.fit(X_train, y_train)\n","benchmarking(\"Random Forest\", rf_grid.best_estimator_, f\"Best N={rf_grid.best_params_['n_estimators']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lv1D8plB3lOG","executionInfo":{"status":"ok","timestamp":1767634695444,"user_tz":-180,"elapsed":42229,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"f7dc18c1-d37b-4e8f-b3ce-dd760283e25b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Finished Random Forest\n"]}]},{"cell_type":"code","source":["# Final Table\n","# Convert the results list to a DataFrame\n","results_df = pd.DataFrame(results)\n","results_df = results_df.sort_values(by=\"Test Accuracy\", ascending=False)\n","print(results_df.to_string(index=False))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3a18qkVK3n8Z","executionInfo":{"status":"ok","timestamp":1767634697060,"user_tz":-180,"elapsed":5,"user":{"displayName":"Cengiz Sarı","userId":"07073946851309561314"}},"outputId":"de2c7406-1d30-4575-bf59-1a3603cfc6c9"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["         Classifier  Training time  Test Accuracy          Hyperparameters/Choices\n","Logistic Regression        0.33282        0.91333                         Best C=1\n","    Soft-margin SVM        0.12495        0.90267                       Best C=0.1\n","         Kernel SVM        0.16696        0.89333 Best {'C': 10, 'gamma': 'scale'}\n","   Non-linear Logit       13.35631        0.88267       Degree=2 Polynomial, C=0.1\n","      Random Forest        4.14986        0.87867                       Best N=200\n","               k-NN        0.00435        0.82533                         Best k=3\n","        Naive Bayes        0.00997        0.75600            Gaussian Distribution\n"]}]},{"cell_type":"markdown","source":["#### **Evaluations of the Results and comparison of PCA vs without PCA**\n","The hyperparameter selections are performed by providing predetermined values to the GridSearch library, which selects the most accurate one by trying each combination.\n","##### **Training Times**\n","With the application of PCA, the feature space was reduced, leading to faster training across all models compared to the original dataset.\n","k-NN and Naive Bayes remain the fastest methods again.\n","Non-linear Logit and Random Forest required the most time again, but PCA effectively decreased the Non-linear Logit time from over 100 seconds to just 13.\n","##### **Accuracy**\n","Some of the information is lost during the projection to a new space due to the reduction of dimensionality.\n","Logistic Regression is the top accurate method with PCA at $91.33%$, suggesting that the principal components retained a strong linear relationship with the target.\n","The largest decrease in performance occurs with the Naive Bayes method, dropping to $75.60%$. We gave the gaussian distribution as the distribution type for the continous parameters, the assumption could not hold for the PCA version."],"metadata":{"id":"-Rm_q5AUPcT2"}},{"cell_type":"markdown","source":[],"metadata":{"id":"bvoFKMREPbEV"}}]}